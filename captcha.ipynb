{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [str(i) for i in range(0, 10)]\n",
    "alphabet = ''.join(source)\n",
    "#img_path = '/Users/fowillwly/Jupyter Notebook/商业大数据分析/data/14_0544.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def img_loader(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    return img.convert('RGB')\n",
    "\n",
    "def make_dataset(data_path, alphabet, num_class, num_char):\n",
    "    img_names = os.listdir(data_path)\n",
    "    samples = []\n",
    "    for img_name in img_names:\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(data_path, img_name)\n",
    "            target_str = img_name.split('_')[1][:4]\n",
    "            assert len(target_str) == num_char\n",
    "            target = []\n",
    "            for char in target_str:\n",
    "                vec = [0] * num_class\n",
    "                vec[alphabet.find(char)] = 1\n",
    "                target += vec\n",
    "            samples.append((img_path, target))\n",
    "    return samples  \n",
    "\n",
    "class CaptchaData(Dataset):\n",
    "    def __init__(self, data_path, num_class=10, num_char=4, \n",
    "                 transform=None, target_transform=None, alphabet=alphabet):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_class = num_class\n",
    "        self.num_char = num_char\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.alphabet = alphabet\n",
    "        self.samples = make_dataset(self.data_path, self.alphabet, \n",
    "                                    self.num_class, self.num_char)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path, target = self.samples[index]\n",
    "        img = img_loader(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, torch.Tensor(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_class=10, num_char=4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.num_char = num_char\n",
    "        self.conv = nn.Sequential(\n",
    "                #batch*3*180*100\n",
    "                nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=(1, 1)),\n",
    "                nn.MaxPool2d(2,2),\n",
    "                nn.BatchNorm2d(num_features=16),\n",
    "                nn.ReLU(),\n",
    "                #batch*16*90*50\n",
    "                nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=(1, 1)),\n",
    "                nn.MaxPool2d(2,2),\n",
    "                nn.BatchNorm2d(num_features=64),\n",
    "                nn.ReLU(),\n",
    "                #batch*64*45*25\n",
    "                nn.Conv2d(in_channels=64, out_channels=512, kernel_size=3, padding=(1, 1)),\n",
    "                nn.MaxPool2d(2,2),\n",
    "                nn.BatchNorm2d(num_features=512),\n",
    "                nn.ReLU(),\n",
    "                #batch*512*22*12\n",
    "                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=(1, 1)),\n",
    "                nn.MaxPool2d(2,2),\n",
    "                nn.BatchNorm2d(num_features=512),\n",
    "                nn.ReLU(),\n",
    "                #batch*512*11*6\n",
    "                )\n",
    "        self.fc = nn.Linear(in_features=512*11*6, out_features=self.num_class*self.num_char)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 512*11*6)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1b8aa3a78ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-1b8aa3a78ce6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "base_lr = 0.001\n",
    "max_epoch = 200\n",
    "model_path = './checkpoints/model.pth'\n",
    "restor = False\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "\n",
    "def calculat_acc(output, target):\n",
    "    output, target = output.view(-1, 10), target.view(-1, 10)\n",
    "    output = nn.functional.softmax(output, dim=1)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    target = torch.argmax(target, dim=1)\n",
    "    output, target = output.view(-1, 4), target.view(-1, 4)\n",
    "    correct_list = []\n",
    "    for i, j in zip(target, output):\n",
    "        if torch.equal(i, j):\n",
    "            correct_list.append(1)\n",
    "        else:\n",
    "            correct_list.append(0)\n",
    "    acc = sum(correct_list) / len(correct_list)\n",
    "    return acc\n",
    "\n",
    "def train():\n",
    "    transforms = Compose([ToTensor()])\n",
    "    train_dataset = CaptchaData('./data/train', transform=transforms)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, \n",
    "                             shuffle=True, drop_last=True)\n",
    "    test_data = CaptchaData('./data/test', transform=transforms)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size, \n",
    "                                  num_workers=0, shuffle=True, drop_last=True)\n",
    "    cnn = CNN()\n",
    "    if torch.cuda.is_available():\n",
    "        cnn.cuda()\n",
    "    if restor:\n",
    "        cnn.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=base_lr)\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        start_ = time.time()\n",
    "        \n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        cnn.train()\n",
    "        for img, target in train_data_loader:\n",
    "            img = Variable(img)\n",
    "            target = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                target = target.cuda()\n",
    "            output = cnn(img)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = calculat_acc(output, target)\n",
    "            acc_history.append(acc)\n",
    "            loss_history.append(loss)\n",
    "        print('train_loss: {:.4}|train_acc: {:.4}'.format(\n",
    "                torch.mean(torch.Tensor(loss_history)),\n",
    "                torch.mean(torch.Tensor(acc_history)),\n",
    "                ))\n",
    "        \n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        cnn.eval()\n",
    "        for img, target in test_data_loader:\n",
    "            img = Variable(img)\n",
    "            target = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                target = target.cuda()\n",
    "            output = cnn(img)\n",
    "            \n",
    "            acc = calculat_acc(output, target)\n",
    "            acc_history.append(acc)\n",
    "            loss_history.append(float(loss))\n",
    "        print('test_loss: {:.4}|test_acc: {:.4}'.format(\n",
    "                torch.mean(torch.Tensor(loss_history)),\n",
    "                torch.mean(torch.Tensor(acc_history)),\n",
    "                ))\n",
    "        print('epoch: {}|time: {:.4f}'.format(epoch, time.time()-start_))\n",
    "        torch.save(cnn.state_dict(), model_path)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
